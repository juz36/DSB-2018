{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nms._ext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-490c0e05c6f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#import coco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/pytorch-mask-rcnn-master/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mroialign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_align\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_and_resize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCropAndResizeFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/pytorch-mask-rcnn-master/nms/nms_wrapper.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpth_nms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpth_nms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/pytorch-mask-rcnn-master/nms/pth_nms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpth_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nms._ext'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import coco\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "#MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "TRAIN_PATH = 'stage1_train/'\n",
    "TEST_PATH = 'stage1_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CellConfig(Config):\n",
    "    \"\"\"Configuration for training on data science bowl 2018 dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the nuclei images dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"cell\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CellDataset(utils.Dataset):\n",
    "    def load_cells(self, mode):\n",
    "        if mode == 'train':\n",
    "            image_dir = os.path.join(TRAIN_PATH)\n",
    "        elif mode == 'test':\n",
    "            image_dir = os.path.join(TEST_PATH)\n",
    "        image_list = sorted(os.listdir(image_dir))\n",
    "        \n",
    "        self.add_class(\"cells\", 1, \"cell\")\n",
    "        \n",
    "        for id_ in image_list:\n",
    "            self.add_image(\n",
    "                \"cells\",\n",
    "                image_id=id_,\n",
    "                path=image_dir+'/'+id_+'/images/',\n",
    "                gt_path=image_dir+'/'+id_+'/masks/'\n",
    "                )\n",
    "            \n",
    "    def load_mask(self, image_id, IMG_HEIGHT=256, IMG_WIDTH=256):\n",
    "        #mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "        mask = []\n",
    "        for mask_file in next(os.walk(image_info['gt_path']))[2]:\n",
    "            mask_ = imread(image_info['gt_path']+mask_file)\n",
    "            mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',\n",
    "                                        preserve_range=True), axis=-1)\n",
    "            mask.append(mask_)\n",
    "        class_ids = np.ones([len(mask)]).astype(np.int32)\n",
    "        mask = np.stack(mask,axis=2)\n",
    "        return mask, class_ids\n",
    "        \n",
    "    def load_image(self, image_id, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=3):\n",
    "        image = imread(self.path + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
    "        image = resize(image, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train = CellDataset()\n",
    "dataset_train.load_cells('train')\n",
    "dataset_train.prepare()\n",
    "dataset_val = CellDataset()\n",
    "dataset_val.load_cells('test')\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = modellib.MaskRCNN(config=config,\n",
    "                                  model_dir=args.logs)\n",
    "if config.GPU_COUNT:\n",
    "        model = model.cuda()\n",
    "\n",
    "model_path = \"\"\n",
    "\n",
    "model.load_weights(model_path)\n",
    "\n",
    "model.train_model(dataset_train, dataset_val,\n",
    "                    learning_rate=config.LEARNING_RATE,\n",
    "                    epochs=40,\n",
    "                    layers='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
