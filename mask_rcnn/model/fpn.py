import torch.nn as nn
import torch.nn.functional as F

class FPN(nn.Module):
    def __init__(self):
        super(FPN, self).__init__()
        self.c5p5 = nn.Conv2d(512 * 4, 256, kernel_size=1, stride=1, padding=0)
        self.c4p4 = nn.Conv2d(256 * 4, 256, kernel_size=1, stride=1, padding=0)
        self.c3p3 = nn.Conv2d(128 * 4, 256, kernel_size=1, stride=1, padding=0)
        self.c2p2 = nn.Conv2d(64 * 4, 256, kernel_size=1, stride=1,  padding=0)

        self.p2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.p3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.p4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.p5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)

        self.p6 = nn.MaxPool2d(kernel_size=1, stride=2, padding=0, ceil_mode=False)

    def forward(self, C2, C3, C4, C5):
        P5 = self.c5p5(C5)
        P4 = self.c4p4(C4) + F.upsample(P5, scale_factor=2, mode='bilinear')
        P3 = self.c3p3(C3) + F.upsample(P4, scale_factor=2, mode='bilinear')
        P2 = self.c2p2(C2) + F.upsample(P3, scale_factor=2, mode='bilinear')

        # Attach 3x3 conv to all P layers to get the final feature maps.
        # P2 is 256, P3 is 128, P4 is 64, P5 is 32
        P2 = self.p2(P2)
        P3 = self.p3(P3)
        P4 = self.p4(P4)
        P5 = self.p5(P5)

        # P6 is used for the 5th anchor scale in RPN. Generated by
        # subsampling from P5 with stride of 2.
        P6 = self.p6(P5)

        return P2, P3, P4, P5, P6
